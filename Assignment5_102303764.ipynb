{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8hOnZH87Z1S6sRRXjspR/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GNtSAAFLhu3",
        "outputId": "ec6b890f-7dd5-4f65-dce4-28cd8703b586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Q1 Result:\n",
            "Best Params: lr=0.1, lam=0\n",
            "Min Cost: 0.004108117326630319\n",
            "Max R2: 0.9958918826733697\n"
          ]
        }
      ],
      "source": [
        "# Q1 - Ridge Regression using Gradient Descent\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate correlated dataset\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 7)\n",
        "X[:, 1] = X[:, 0] + np.random.normal(0, 0.02, 100)\n",
        "X[:, 2] = X[:, 0] + np.random.normal(0, 0.03, 100)\n",
        "y = 3*X[:,0] + 2*X[:,1] + np.random.normal(0, 0.1, 100)\n",
        "\n",
        "# Scale features for stable gradient descent\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "y = (y - y.mean()) / y.std()\n",
        "\n",
        "# Ridge Regression using Gradient Descent\n",
        "def ridge_regression(X, y, lr, lam, epochs=1000):\n",
        "    n, m = X.shape\n",
        "    X_b = np.c_[np.ones((n,1)), X]\n",
        "    theta = np.zeros((m+1,1))\n",
        "    y = y.reshape(-1,1)\n",
        "    for _ in range(epochs):\n",
        "        gradients = (1/n)*X_b.T.dot(X_b.dot(theta)-y) + (lam/n)*np.r_[[[0]],theta[1:]]\n",
        "        theta -= lr * gradients\n",
        "        theta = np.clip(theta, -1e5, 1e5)\n",
        "    y_pred = X_b.dot(theta)\n",
        "    cost = np.mean((y_pred - y)**2) + lam*np.sum(theta[1:]**2)\n",
        "    return theta, cost, r2_score(y, y_pred)\n",
        "\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
        "lambdas = [1e-5, 1e-3, 0, 1, 10]\n",
        "\n",
        "best = (None, float('inf'), -1)\n",
        "for lr in learning_rates:\n",
        "    for lam in lambdas:\n",
        "        theta, cost, r2 = ridge_regression(X, y, lr, lam)\n",
        "        if np.isfinite(cost) and np.isfinite(r2):\n",
        "            if cost < best[1] and r2 > best[2]:\n",
        "                best = (f\"lr={lr}, lam={lam}\", cost, r2)\n",
        "\n",
        "print(\"# Q1 Result:\")\n",
        "print(\"Best Params:\", best[0])\n",
        "print(\"Min Cost:\", best[1])\n",
        "print(\"Max R2:\", best[2])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2 - Linear, Ridge, and Lasso Regression on Hitters Dataset\n",
        "\n",
        "# (a) Pre-process the data (null values, noise, categorical to numerical encoding)\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "# Load the Hitters dataset from Google Drive\n",
        "# (Note: The dataset link provided in the assignment is no longer accessible)\n",
        "\n",
        "url = \"https://drive.google.com/uc?id=1qzCKF6JKKMB0p7ul_lLy8tdmRk3vE_bG\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# (a) Handle missing values and convert categorical to numeric\n",
        "data = data.dropna()\n",
        "for col in data.select_dtypes(include='object'):\n",
        "    data[col] = LabelEncoder().fit_transform(data[col])\n",
        "\n",
        "# (b) Separate input and output features and perform scaling\n",
        "X = data.drop('Salary', axis=1)\n",
        "y = data['Salary']\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# (c) Fit Linear, Ridge, and Lasso regression models\n",
        "# Regularization parameter (alpha) = 0.5748\n",
        "linear_model = LinearRegression()\n",
        "ridge_model = Ridge(alpha=0.5748)\n",
        "lasso_model = Lasso(alpha=0.5748)\n",
        "\n",
        "linear_model.fit(X_train, y_train)\n",
        "ridge_model.fit(X_train, y_train)\n",
        "lasso_model.fit(X_train, y_train)\n",
        "\n",
        "# (d) Evaluate model performance\n",
        "linear_pred = linear_model.predict(X_test)\n",
        "ridge_pred = ridge_model.predict(X_test)\n",
        "lasso_pred = lasso_model.predict(X_test)\n",
        "\n",
        "linear_r2 = r2_score(y_test, linear_pred)\n",
        "ridge_r2 = r2_score(y_test, ridge_pred)\n",
        "lasso_r2 = r2_score(y_test, lasso_pred)\n",
        "\n",
        "linear_mse = mean_squared_error(y_test, linear_pred)\n",
        "ridge_mse = mean_squared_error(y_test, ridge_pred)\n",
        "lasso_mse = mean_squared_error(y_test, lasso_pred)\n",
        "\n",
        "# Print the results (you don't need to run this cell)\n",
        "print(\"# Q2 Results:\")\n",
        "print(\"Linear Regression -> R2:\", linear_r2, \" MSE:\", linear_mse)\n",
        "print(\"Ridge Regression  -> R2:\", ridge_r2, \" MSE:\", ridge_mse)\n",
        "print(\"Lasso Regression  -> R2:\", lasso_r2, \" MSE:\", lasso_mse)"
      ],
      "metadata": {
        "id": "MjZPw5JbaS6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3 - Cross Validation for Ridge and Lasso Regression\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.linear_model import RidgeCV, LassoCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "# Load California Housing dataset (modern replacement for Boston)\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split and scale\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# RidgeCV\n",
        "ridgecv = RidgeCV(alphas=[0.01, 0.1, 1, 10, 100], cv=5)\n",
        "ridgecv.fit(X_train, y_train)\n",
        "ridge_y_pred = ridgecv.predict(X_test)\n",
        "\n",
        "# LassoCV\n",
        "lassocv = LassoCV(alphas=[0.01, 0.1, 1, 10, 100], cv=5, max_iter=10000)\n",
        "lassocv.fit(X_train, y_train)\n",
        "lasso_y_pred = lassocv.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "results = {\n",
        "    \"Model\": [\"RidgeCV\", \"LassoCV\"],\n",
        "    \"Best Alpha\": [ridgecv.alpha_, lassocv.alpha_],\n",
        "    \"R2 Score\": [r2_score(y_test, ridge_y_pred), r2_score(y_test, lasso_y_pred)],\n",
        "    \"MSE\": [mean_squared_error(y_test, ridge_y_pred), mean_squared_error(y_test, lasso_y_pred)]\n",
        "}\n",
        "\n",
        "print(\"# Q3 Results:\")\n",
        "print(pd.DataFrame(results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fs8lzyrXZmxJ",
        "outputId": "ae8266d5-302c-4a25-bb5b-6f4017707c6b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Q3 Results:\n",
            "     Model  Best Alpha  R2 Score       MSE\n",
            "0  RidgeCV        0.01  0.575788  0.555891\n",
            "1  LassoCV        0.01  0.581615  0.548255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4 - Multiclass Logistic Regression using One-vs-Rest\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split and scale\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train One-vs-Rest Logistic Regression\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"# Q4 Results:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=iris.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtaFVUnKaKh1",
        "outputId": "47cc9122-a207-42fe-9b0a-32b51fe79696"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Q4 Results:\n",
            "Accuracy: 0.9666666666666667\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      0.89      0.94         9\n",
            "   virginica       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}